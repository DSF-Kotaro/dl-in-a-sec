{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ihqvJ-pkz5Qe"
   },
   "source": [
    "##### Copyright 2018 The TensorFlow Hub Authors.\n",
    "\n",
    "##### Modifications Copyright 2019 Tomoaki Masuda.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6pZZwchgz5Qg"
   },
   "outputs": [],
   "source": [
    "# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このノートブックは、以下のノートブックを元に日本語訳、一部章立ての再構成、加筆を行いました。https://github.com/tensorflow/hub/blob/master/examples/colab/action_recognition_with_tf_hub.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lxx2zoo7z5Qj"
   },
   "source": [
    "\n",
    "#  TF-Hub 動画のアクション認識モデル\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mtWjLaXdz5Ql"
   },
   "source": [
    "\n",
    "このノートブックでは、[tfhub.dev/deepmind / i3d-kinetics-400/1](https://tfhub.dev/deepmind/i3d-kinetics-400/1)モジュールを使って、動画データからのアクション認識を試します。\n",
    "\n",
    "基礎となるモデルは、Joao\n",
    "Carreira氏とAndrew Zisserman氏による \"[Quo Vadis, Action Recognition? A New\n",
    "Model and the Kinetics Dataset](https://arxiv.org/abs/1705.07750)\" で説明されています。\n",
    "この論文は2017年5月にarXivに投稿され、CVPR 2017論文として発表されました。 ソースコードは [github](https://github.com/deepmind/kinetics-i3d) で公開されています。 \n",
    "\n",
    " \"Quo Vadis\"では、動画分類のための新しいアーキテクチャ、Inflated 3D Convnet、略してI3Dを導入しました。このアーキテクチャをファインチューニング（転移学習）したもので、UCF101データセット、およびHMDB51データセットで最高精度を達成しました。\n",
    "\n",
    "Kineticsで事前学習したI3Dモデルは、CVPR 2017 [Charades challenge](http://vuchallenge.org/charades.html)でも優勝しました。\n",
    "\n",
    "元のモジュールは[kinetics-400データセット](https://deepmind.com/research/open-source/open-source-datasets/kinetics/) で学習し、約400の異なる動作を識別できます。これらのアクションのラベルは、[label map file](https://github.com/deepmind/kinetics-i3d/blob/master/data/label_map.txt) で確認できます。\n",
    "\n",
    "このノートブックでは、UCF101データセットのビデオ内の動作を認識します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DQsl_ljgz5Qm"
   },
   "source": [
    "\n",
    "## A. 環境を準備する\n",
    "\n",
    "必要なライブラリのインストール、インポートを行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3PsXQJlKz5Qn"
   },
   "outputs": [],
   "source": [
    "#added on 2020/6/20\n",
    "%tensorflow_version 1.x\n",
    "\n",
    "# Install the necessary python packages.\n",
    "!pip install -q \"tensorflow>=1.7\" \"tensorflow-hub\" \"imageio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C2bnddo4z5Qo"
   },
   "outputs": [],
   "source": [
    "#@title Import the necessary modules\n",
    "# TensorFlow and TF-Hub modules.\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Some modules to help with reading the UCF101 dataset.\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import tempfile\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Some modules to display an animation using imageio.\n",
    "import imageio\n",
    "from IPython import display\n",
    "\n",
    "from urllib import request  # requires python3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## B. データセットを準備する\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. UCF101データセットをダウンロードする\n",
    "\n",
    "UCF101データセットをダウンロードします。オリジナルのノートブックはリンク切れとなっていたため、変更を行なっています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GSDD6x6Gz5Qq"
   },
   "outputs": [],
   "source": [
    "#@title Helper functions for the UCF101 dataset\n",
    "\n",
    "# Utilities to fetch videos from UCF101 dataset\n",
    "UCF_ROOT = \"http://crcv.ucf.edu/THUMOS14/UCF101/UCF101/\"\n",
    "_VIDEO_LIST = None\n",
    "_CACHE_DIR = tempfile.mkdtemp()\n",
    "\n",
    "def list_ucf_videos():\n",
    "  \"\"\"Lists videos available in UCF101 dataset.\"\"\"\n",
    "  global _VIDEO_LIST\n",
    "  if not _VIDEO_LIST:\n",
    "    index = request.urlopen(UCF_ROOT).read().decode(\"utf-8\")\n",
    "    videos = re.findall(\"(v_[\\w_]+\\.avi)\", index)\n",
    "    _VIDEO_LIST = sorted(set(videos))\n",
    "  return list(_VIDEO_LIST)\n",
    "\n",
    "def fetch_ucf_video(video):\n",
    "  \"\"\"Fetchs a video and cache into local filesystem.\"\"\"\n",
    "  cache_path = os.path.join(_CACHE_DIR, video)\n",
    "  if not os.path.exists(cache_path):\n",
    "    urlpath = request.urljoin(UCF_ROOT, video)\n",
    "    print(\"Fetching %s => %s\" % (urlpath, cache_path))\n",
    "    data = request.urlopen(urlpath).read()\n",
    "    open(cache_path, \"wb\").write(data)\n",
    "  return cache_path\n",
    "\n",
    "# Utilities to open video files using CV2\n",
    "def crop_center_square(frame):\n",
    "  y, x = frame.shape[0:2]\n",
    "  min_dim = min(y, x)\n",
    "  start_x = (x // 2) - (min_dim // 2)\n",
    "  start_y = (y // 2) - (min_dim // 2)\n",
    "  return frame[start_y:start_y+min_dim,start_x:start_x+min_dim]\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(224, 224)):\n",
    "  cap = cv2.VideoCapture(path)\n",
    "  frames = []\n",
    "  try:\n",
    "    while True:\n",
    "      ret, frame = cap.read()\n",
    "      if not ret:\n",
    "        break\n",
    "      frame = crop_center_square(frame)\n",
    "      frame = cv2.resize(frame, resize)\n",
    "      frame = frame[:, :, [2, 1, 0]]\n",
    "      frames.append(frame)\n",
    "      \n",
    "      if len(frames) == max_frames:\n",
    "        break\n",
    "  finally:\n",
    "    cap.release()\n",
    "  return np.array(frames) / 255.0\n",
    "\n",
    "def animate(images):\n",
    "  converted_images = np.clip(images * 255, 0, 255).astype(np.uint8)\n",
    "  imageio.mimsave('./animation.gif', converted_images, fps=25)\n",
    "  with open('./animation.gif','rb') as f:\n",
    "      display.display(display.Image(data=f.read(), height=300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. kinetics-400のラベルを取得する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F8lg_FvQz5Qs"
   },
   "outputs": [],
   "source": [
    "#@title Get the kinetics-400 labels\n",
    "# Get the kinetics-400 action labels from the GitHub repository.\n",
    "KINETICS_URL = \"https://raw.githubusercontent.com/deepmind/kinetics-i3d/master/data/label_map.txt\"\n",
    "with request.urlopen(KINETICS_URL) as obj:\n",
    "  labels = [line.decode(\"utf-8\").strip() for line in obj.readlines()]\n",
    "print(\"Found %d labels.\" % len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0B14rAPz5Qw"
   },
   "source": [
    "\n",
    "### 3. データセットの中身を見てみる\n",
    "\n",
    "まず、UCF101データセットに含まれる動画のリストを取得します。さらに、サンプルの動画を再生してみましょう。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ODlooVeMz5Qx"
   },
   "outputs": [],
   "source": [
    "# Get the list of videos in the dataset.\n",
    "ucf_videos = list_ucf_videos()\n",
    "  \n",
    "categories = {}\n",
    "for video in ucf_videos:\n",
    "  category = video[2:-12]\n",
    "  if category not in categories:\n",
    "    categories[category] = []\n",
    "  categories[category].append(video)\n",
    "print(\"Found %d videos in %d categories.\" % (len(ucf_videos), len(categories)))\n",
    "\n",
    "for category, sequences in categories.items():\n",
    "  summary = \", \".join(sequences[:2])\n",
    "  print(\"%-20s %4d videos (%s, ...)\" % (category, len(sequences), summary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2PREUwgHz5Qz"
   },
   "outputs": [],
   "source": [
    "# Get a sample cricket video.\n",
    "sample_video = load_video(fetch_ucf_video(\"v_CricketShot_g04_c02.avi\"))\n",
    "\n",
    "print(\"sample_video is a numpy array of shape %s.\" % str(sample_video.shape))\n",
    "animate(sample_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. データセットを前処理する（不要）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## D. モデルを取得する\n",
    "\n",
    "TF-Hubから、学習済みのI3Dモデルを取得します。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. モデルを学習させる\n",
    "\n",
    "学習済みモデルを使うため、ここでは行いません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F. 学習済みモデルを評価する\n",
    "\n",
    "先ほど取得した学習済みモデルを使って、指定した動画の動作カテゴリを認識してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1_Fo1pn3z5Q2"
   },
   "outputs": [],
   "source": [
    "# Run the i3d model on the video and print the top 5 actions.\n",
    "\n",
    "# First add an empty dimension to the sample video as the model takes as input\n",
    "# a batch of videos.\n",
    "model_input = np.expand_dims(sample_video, axis=0)\n",
    "\n",
    "# Create the i3d model and get the action probabilities.\n",
    "with tf.Graph().as_default():\n",
    "  i3d = hub.Module(\"https://tfhub.dev/deepmind/i3d-kinetics-400/1\")\n",
    "  input_placeholder = tf.placeholder(shape=(None, None, 224, 224, 3), dtype=tf.float32)\n",
    "  logits = i3d(input_placeholder)\n",
    "  probabilities = tf.nn.softmax(logits)\n",
    "  with tf.train.MonitoredSession() as session:\n",
    "    [ps] = session.run(probabilities,\n",
    "                       feed_dict={input_placeholder: model_input})\n",
    "\n",
    "print(\"Top 5 actions:\")\n",
    "for i in np.argsort(ps)[::-1][:5]:\n",
    "  print(\"%-22s %.2f%%\" % (labels[i], ps[i] * 100))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "動画のコンテキスト認識を行う",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}