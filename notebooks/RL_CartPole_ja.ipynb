{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "バランス制御を学習する",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XumXgZKAQU3l"
      },
      "source": [
        "# Cartpoleを試す\n",
        "\n",
        "Stable BaselinesとRL Baselines Zooを用いて、強化学習によるバランス制御を試します。\n",
        "このノートブックは以下の内容を含みます。\n",
        "\n",
        "- 環境準備\n",
        "- Gym環境とエージェントを作成\n",
        "- エージェントの学習と評価\n",
        "- 1行のコマンドで学習\n",
        "- リプレイ動画の生成\n",
        "\n",
        "なお、GIFアニメーションによる学習前後のプレイの可視化は、ikeyasu氏の[ChainerRL を Colaboratory で動かす - Qiita](https://qiita.com/ikeyasu/items/ec3c88ce13a2d5e41f26) を参考として作成しました。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C-T7Ib9WQ7vD"
      },
      "source": [
        "## A. 環境を準備する\n",
        "\n",
        "Stable Baselinesと依存ライブラリをインストールします。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CPOgmLm3iGXA"
      },
      "source": [
        "### 1. 必要なライブラリのインストール\n",
        "\n",
        "インストールに5分程度を要します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# added on 2020/2/9\n",
        "%tensorflow_version 1.x\n",
        "# エラー解消のため最新のstable-baselinesをインストール\n",
        "!pip install optuna\n",
        "!git clone https://github.com/hill-a/stable-baselines\n",
        "%cd stable-baselines\n",
        "!pip install -e .[docs,tests]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "503Gi2076F7u"
      },
      "outputs": [],
      "source": [
        "!apt-get -y install swig xvfb python-opengl\n",
        "!pip install box2d box2d-kengz pybullet pyyaml pytablewriter pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FtY8FhliLsGm"
      },
      "source": [
        "### 2. 必要なPythonライブラリのインポート\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "BIedd7Pz9sOs"
      },
      "outputs": [],
      "source": [
        "import time, os, gym\n",
        "import numpy as np\n",
        "\n",
        "from stable_baselines.deepq.policies import MlpPolicy\n",
        "from stable_baselines.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
        "from stable_baselines.common import set_global_seeds\n",
        "from stable_baselines import DQN\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t5WNF6G5gWZ1"
      },
      "source": [
        "## B. Gym環境を準備する\n",
        "\n",
        "CartPole-v1という、OpenAI Gymの中の、棒のバランスをとるタスク用の環境を準備します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "TgjfyOTPVxG6"
      },
      "outputs": [],
      "source": [
        "env_id = \"CartPole-v1\"\n",
        "env = gym.make(env_id)\n",
        "env = DummyVecEnv([lambda: env])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qJzpXfLEiGXM"
      },
      "source": [
        "## C. その他の処理を準備する\n",
        "\n",
        "Gym環境以外のヘルパー関数などを準備します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o8MYIWs6ReFe"
      },
      "source": [
        "### 1. 評価方法の準備\n",
        "\n",
        "次の関数は、強化学習のエージェントを、環境内で指定のステップ分動かし、その結果得られる報酬を算出します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "63M8mSKR-6Zt"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, num_steps=1000):\n",
        "    \"\"\"\n",
        "    Evaluate a RL agent\n",
        "    :param model: (BaseRLModel object) the RL Agent\n",
        "    :param num_steps: (int) number of timesteps to evaluate it\n",
        "    :return: (float) Mean reward\n",
        "    \"\"\"\n",
        "    episode_rewards = [[0.0] for _ in range(env.num_envs)]\n",
        "    obs = env.reset()\n",
        "    for i in range(num_steps):\n",
        "      # _states are only useful when using LSTM policies\n",
        "      actions, _states = model.predict(obs)\n",
        "      # here, action, rewards and dones are arrays\n",
        "      # because we are using vectorized env\n",
        "      obs, rewards, dones, info = env.step(actions)\n",
        "      \n",
        "      # Stats\n",
        "      for i in range(env.num_envs):\n",
        "          episode_rewards[i][-1] += rewards[i]\n",
        "          if dones[i]:\n",
        "              episode_rewards[i].append(0.0)\n",
        "\n",
        "    mean_rewards =  [0.0 for _ in range(env.num_envs)]\n",
        "    n_episodes = 0\n",
        "    for i in range(env.num_envs):\n",
        "        mean_rewards[i] = np.mean(episode_rewards[i])     \n",
        "        n_episodes += len(episode_rewards[i])   \n",
        "\n",
        "    # Compute mean reward\n",
        "    mean_reward = round(np.mean(mean_rewards), 1)\n",
        "    print(\"Mean reward:\", mean_reward, \"Num episodes:\", n_episodes)\n",
        "\n",
        "    return mean_reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5eHJHMwhRbbx"
      },
      "source": [
        "### 2. プレイ動画の再生用関数\n",
        "\n",
        "次に、仮想ディプレイを利用し、Colaboratory上でエージェントの振る舞いをアニメーションで見られるようにする関数を定義します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "both",
        "colab": {},
        "colab_type": "code",
        "id": "NQgeFoi7OjdW"
      },
      "outputs": [],
      "source": [
        "def playback(model, env, maxsteps):\n",
        "  # Start virtual display\n",
        "  display = Display(visible=0, size=(1024, 768))\n",
        "  display.start()\n",
        "\n",
        "  os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)\n",
        "\n",
        "  frames = []\n",
        "  for i in range(3):\n",
        "      obs = env.reset()\n",
        "      done = False\n",
        "      R = 0\n",
        "      t = 0\n",
        "      while not done and t < maxsteps:\n",
        "          frames.append(env.render(mode = 'rgb_array'))\n",
        "          action, _states = model.predict(obs)        \n",
        "          obs, rewards, dones, info = env.step(action)\n",
        "          R += rewards\n",
        "          t += 1\n",
        "      print('test episode:', i, 'R:', R)\n",
        "  #    model.stop_episode()\n",
        "  #env.render()\n",
        "\n",
        "  return frames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mV4E26FOiGXN"
      },
      "source": [
        "## D. モデルを作成する\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "leO3palKRcKe"
      },
      "source": [
        "### 1. エージェントの準備\n",
        "\n",
        "さきほど `env` に設定した `CartPole-v1` 環境を使って学習する、DQNのエージェントを作成します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "pUWGZp3i9wyf"
      },
      "outputs": [],
      "source": [
        "model = DQN(MlpPolicy, env, verbose=1, tensorboard_log=\"./cartpole_tensorboard/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rtnloDPgRgwb"
      },
      "source": [
        "### 3. 学習前エージェントの評価\n",
        "\n",
        "さきほど設定した評価関数を、学習前のまっさらなエージェントに対して適用してみましょう。\n",
        "\n",
        "実行すると平均の報酬(Mean reward)と、エピソード数(Num episodes)が得られます。学習後の実行と比べてみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "xDHLMA6NFk95"
      },
      "outputs": [],
      "source": [
        "# Random Agent, before training\n",
        "mean_reward_before_train = evaluate(model, num_steps=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JUDfmE0kSn-F"
      },
      "source": [
        "### 4. 未学習状態での振る舞いを見る"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0G9L9-ioO4QK"
      },
      "source": [
        "この未学習状態でのプレイぶりを再生してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "oMyUhOYkO4fp"
      },
      "outputs": [],
      "source": [
        "frames = playback(model, env, 50)\n",
        "\n",
        "plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
        "patch = plt.imshow(frames[0])\n",
        "plt.axis('off')\n",
        "animate = lambda i: patch.set_data(frames[i])\n",
        "ani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval = 50)\n",
        "HTML(ani.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8BeoCsVLSsy4"
      },
      "source": [
        "すぐに棒が倒れてしまい、プレイがあまり継続しないことがわかります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r5UoXTZPNdFE"
      },
      "source": [
        "## E. エージェントを学習させる\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2MGKPFywLutx"
      },
      "source": [
        "### 1. Tensorboardのセットアップ\n",
        "\n",
        "学習の経過をモニタするため、Tensorboardをセットアップします。セルの出力に、インラインで表示することができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "mkQjd6unLt34"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir cartpole_tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z4xelDCZLwci"
      },
      "source": [
        "### 2. エージェントを学習させる\n",
        "\n",
        "試しに1000ステップ学習を進めてみます。所要時間は1分程度です。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "e4cfSXIB-pTF"
      },
      "outputs": [],
      "source": [
        "n_timesteps = 5000\n",
        "model.learn(n_timesteps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9Io2S8HniGXn"
      },
      "source": [
        "## F. 学習済みエージェントの評価"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PohU-H2MiGXp"
      },
      "source": [
        "### 1. 学習済みエージェントの評価\n",
        "\n",
        "学習後の平均の報酬(Mean reward)と、エピソード数(Num episodes)が得られます。学習前と比べてみましょう。報酬は大きく上昇し、エピソード数は減少しているのがわかるでしょうか。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ygl_gVmV_QP7"
      },
      "outputs": [],
      "source": [
        "# Evaluate the trained agent\n",
        "mean_reward = evaluate(model, num_steps=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eiUFcGNtiGXs"
      },
      "source": [
        "### 2. 学習済みエージェントの振る舞いを見る\n",
        "\n",
        "実際の振る舞いを再生してみましょう。うまくバランスを制御できていることが分かります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "z_GLm3n3iGXt"
      },
      "outputs": [],
      "source": [
        "frames = playback(model, env, 100)\n",
        "\n",
        "plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
        "patch = plt.imshow(frames[0])\n",
        "plt.axis('off')\n",
        "animate = lambda i: patch.set_data(frames[i])\n",
        "ani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval = 50)\n",
        "HTML(ani.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mJ6N_PdsSUkA"
      },
      "source": [
        "### 3. 追加の学習を続け、振る舞いを見る\n",
        "\n",
        "さらに、10,000ステップ追加で学習してみます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "WxpUtp6iSfHO"
      },
      "outputs": [],
      "source": [
        "n_timesteps = 10000\n",
        "model.learn(n_timesteps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "sYxn3PrrStcF"
      },
      "outputs": [],
      "source": [
        "frames = playback(model, env, 100)\n",
        "\n",
        "plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
        "patch = plt.imshow(frames[0])\n",
        "plt.axis('off')\n",
        "animate = lambda i: patch.set_data(frames[i])\n",
        "ani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval = 50)\n",
        "HTML(ani.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FqMJmts3S3dj"
      },
      "source": [
        "さきほどと振る舞いは変わったのですが、どうも変な学習をしてしまったようです。\n",
        "\n",
        "というのも、エピソードの開始とともに、片方向に移動し、その後もう片方向にずっと移動する、という同じスタイルが連続しています。\n",
        "確かに、こうすることで一定時間プレイを続けて報酬を稼げるのですが、画面から出てしまうとそのエピソードが終了してしまうため、最適な状態は学習できていません。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RBd-p-rAiv-d"
      },
      "source": [
        "# 発展\n",
        "\n",
        "ここまで見てきたように、強化学習は、\n",
        "\n",
        "- 環境を準備する\n",
        "- エージェントを作成する\n",
        "- 環境上で、エージェントに探索させる\n",
        "\n",
        "というステップで学習を進めます。\n",
        "\n",
        "RL Baselines Zooは、これらのステップを1行のコマンドの裏側にまとめてくれています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jR6hlXweTWxe"
      },
      "source": [
        "## RL Baseline Zooのインストール\n",
        "\n",
        "予め学習が収束するようハイパーパラメータなどが準備された1行のコマンドで、上記の学習や動画の生成を試してみましょう。また、DQNだけでなく、さまざまなエージェントのモデルを試すことができます。\n",
        "\n",
        "まず、下のコマンドで、GitHubのリポジトリからrl-baselines-zooプロジェクトをコピーしてきます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "NPAlwySITXzV"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/araffin/rl-baselines-zoo.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yHjMHe0jTMT9"
      },
      "source": [
        "次のコマンドで学習を進められます。実行すると、 `logs/dqn/CartPole-v1_1/` に  `CartPole-v1.pkl` として、学習済みエージェントのモデルが保存されます。\n",
        "\n",
        "一般に、 `logs/{algorithm}/{env_name}_n` に、 `{env_name}.pkl` として保存されます。実行するごとに、n=1, 2, 3... と数が増加します。\n",
        "\n",
        "また、RL Baselines Zooのリポジトリからは、 `/content/rl-baselines-zoo/trained_agents/` 以下に、各モデル・各環境の学習済みモデルが提供されています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "pTiYTFHWiGXl"
      },
      "outputs": [],
      "source": [
        "%cd /content/rl-baselines-zoo\n",
        "!python train.py --algo dqn --env CartPole-v1 --tensorboard-log cartpole_tensorboard/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_HYdeBqbj9C9"
      },
      "source": [
        "### 学習済みエージェントを読み込み、プレイバック\n",
        "\n",
        "では、学習結果を見てみましょう。学習を複数回実行した場合は、`logs/dqn/CartPole-v1_n/CartPole-v1.zip` の、nの数を変えてください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "yVjOpTZGj-ql"
      },
      "outputs": [],
      "source": [
        "model = DQN.load(\"/content/rl-baselines-zoo/logs/dqn/CartPole-v1_1/CartPole-v1.zip\")\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "env = DummyVecEnv([lambda: env]) \n",
        "model.set_env(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Jf5PKdUCTd2q"
      },
      "source": [
        "### プレイ動画の再生\n",
        "\n",
        "学習結果を再生してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "y_np_Va_kjk8"
      },
      "outputs": [],
      "source": [
        "frames = playback(model, env, 500)\n",
        "\n",
        "plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
        "patch = plt.imshow(frames[0])\n",
        "plt.axis('off')\n",
        "animate = lambda i: patch.set_data(frames[i])\n",
        "ani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval = 50)\n",
        "HTML(ani.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J7oZG13dkmbq"
      },
      "source": [
        "## まとめと発展\n",
        "\n",
        "このノートブックでは、CartPoleという基本のタスクを元に強化学習の流れを体感しました。\n",
        "\n",
        "また、Stable Baselinesをそのまま使う場合と、RL Baselines Zooの1行コマンドで学習させる簡易な方法の2つを学びました。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "RG_1UdQbk15u"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}